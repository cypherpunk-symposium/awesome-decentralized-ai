# (quintessential) papers to read later (hopefully someday)

<br>

## å½±æ£® content

<br>

- ðŸŒ² [classic papers](#ðŸŒ²-classic-papers)
- ðŸŒ² [parallel and decentralized training](#ðŸŒ²-parallel-and-decentralized-training)
- ðŸŒ² [models](#ðŸŒ²-models)
- ðŸŒ² [fine-tuning](#ðŸŒ²-fine-tuning)
- ðŸŒ² [ai and ml on blockchains](#ðŸŒ²-ai-and-ml-on-blockchains)
- ðŸŒ² [privacy and sybil protection](#ðŸŒ²-privacy-and-sybil-protection)
- ðŸŒ² [books and the underlying math](#ðŸŒ²-books-and-the-underlying-math)

<br>

---

## ðŸŒ² classic papers

<br>

- [attention is all you need](https://arxiv.org/abs/1706.03762)
- [bert: pre-training of deep bidirectional transformers for language understanding](https://arxiv.org/abs/1810.04805)
- [neural machine translation by jointly learning to align and translate](https://arxiv.org/abs/1409.0473)
- [language models are few-shot learners](https://arxiv.org/abs/2005.14165)
- [huggingface's transformers: state-of-the-art natural language processing](https://arxiv.org/abs/1910.03771)
- [llama: open and efficient foundation language models](https://arxiv.org/abs/2302.13971)

<br>

---

## ðŸŒ² parallel and decentralized training

<br>

- [pytorch fsdp: experiences on scaling fully sharded data parallel](https://arxiv.org/pdf/2304.11277)
- [decentralized training of foundation models in heterogeneous environments](https://arxiv.org/pdf/2206.01288)
- [deepmind's dipaco: distributed path composition](https://arxiv.org/pdf/2403.10616)
- [towards crowdsourced training of large neural networks using decentralized mixture-of-experts](https://arxiv.org/pdf/2002.04013)
- [a comprehensive survey of continual learning: theory, method and application](https://arxiv.org/pdf/2302.00487)
- [model-agnostic meta-learning for fast adaptation of deep networks](https://arxiv.org/pdf/1703.03400)
- [the future of large language model pre-training is federated](https://arxiv.org/abs/2405.10853v2)
- [asynchronous local-sgd training for language modeling](https://arxiv.org/pdf/2401.09135)
- [lo-fi: distributed fine-tuning without communication](https://arxiv.org/pdf/2210.11948)
- [scaling up test-time compute with latent reasoning: a recurrent depth approach](https://arxiv.org/abs/2502.05171)
- [training large language models to reason in a continuous latent space](https://arxiv.org/abs/2412.06769)
- [protocol learning, decentralized frontier risk and the no-off problem](https://arxiv.org/pdf/2412.07890)
- [distributed training of large language models: a comprehensive survey](https://arxiv.org/abs/2401.17555)

<br>

---

### 3d parallelism and transformer architecture

<br>

- [reformer: the efficient transformer](https://arxiv.org/abs/2001.04451)
- [a deep dive into 3d parallelism with nanotron](https://tj-solergibert.github.io/post/3d-parallelism/)
- [megatron-lm: training multi-billion parameter language models using model parallelism](https://arxiv.org/abs/1909.08053)
- [scaling language model training to a trillion parameters using megatron](https://developer.nvidia.com/blog/scaling-language-model-training-to-a-trillion-parameters-using-megatron/)
- [torchgpipe: on-the-fly pipeline parallelism for training giant models](https://arxiv.org/abs/2004.09910)
- [zero: memory optimizations toward training trillion parameter models](https://arxiv.org/abs/1910.02054)
- [gshard: scaling giant models with conditional computation and automatic sharding](https://arxiv.org/abs/2006.16668)
- [spatio-temporal dual affine differential invariant for skeleton-based action recognition](https://arxiv.org/abs/2004.09802)
- [efficient large-scale language model training on gpu clusters using megatron-lm](https://arxiv.org/abs/2104.04473)
- [pipedream: fast and efficient pipeline parallel dnn training](https://arxiv.org/abs/1811.06965)
- [mesh-tensorflow: deep learning for supercomputers](https://arxiv.org/abs/1806.03377)
- [flash attention: fast and memory-efficient exact attention with io-awareness](https://arxiv.org/abs/2205.14135)

<br>

---

## ðŸŒ² models

<br>

- [deepseekmath: pushing the limits of mathematical reasoning in open language models](https://arxiv.org/abs/2402.03300)
- [training large language models to reason in a continuous latent space](https://arxiv.org/abs/2412.06769)
- [scaling up test-time compute with latent reasoning: a recurrent depth approach](https://arxiv.org/abs/2502.05171)
- [do all languages cost the same? tokenization in the era of commercial language models](https://arxiv.org/abs/2305.13707)
- [mixtral of experts](https://arxiv.org/abs/2401.04088)
- [gemini: a family of highly capable multimodal models](https://arxiv.org/abs/2312.11805)

<br>

---

## ðŸŒ² fine-tuning

<br>

- [direct preference optimization: your language model is secretly a reward model](https://arxiv.org/pdf/2305.18290)
- [qlora: efficient finetuning of quantized llms](https://arxiv.org/abs/2305.14314)
- [parameter-efficient fine-tuning of large language models](https://arxiv.org/abs/2401.17555)

<br>

---

## ðŸŒ² ai and ml on blockchains

<br>

- [opml: optimistic machine learning on blockchain](https://arxiv.org/abs/2401.17555)
- [decentralized machine learning: fundamentals, state of the art, and future directions](https://arxiv.org/abs/2301.04820)
- [blockchain-based federated learning: a comprehensive survey](https://arxiv.org/abs/2304.09858)

<br>

---

## ðŸŒ² privacy and sybil protection

<br>

- [verifiable evaluations of machine learning models using zksnarks](https://arxiv.org/pdf/2402.02675)
- [differential privacy for machine learning: a survey](https://arxiv.org/abs/2303.00654)
- [secure aggregation for federated learning: a survey](https://arxiv.org/abs/2304.09858)

<br>

---

## ðŸŒ² books and the underlying math

<br>

- [pattern recognition and machine learning](https://link.springer.com/book/9780387310732)
- [convex optimization](https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf)
- [deep learning](https://www.deeplearningbook.org/)

<br>

---
