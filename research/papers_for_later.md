## papers to read later (hopefully)

<br>


### ml on blockchains

<br>

* [opml: optimistic machine learning on blockchain](https://arxiv.org/pdf/2401.17555)

<br>

---

### parallel and decentralized training

<br>

* [swarm parallelism: training large models
can be surprisingly communication-efficient](https://arxiv.org/pdf/2301.11913)
* [deepmind's diloco: distributed low-communication
training of language models](https://arxiv.org/pdf/2311.08105)
* [decentralized training of foundation models in
heterogeneous environments](https://arxiv.org/pdf/2206.01288)
* [deepmind's dipaco: distributed path composition](https://arxiv.org/pdf/2403.10616)
* [towards crowdsourced training of large neural
networks using decentralized mixture-of-experts](https://arxiv.org/pdf/2002.04013)
* [deepmind's dipaco: distributed path composition](https://arxiv.org/pdf/2403.10616)
* [decentralized training of foundation models in
heterogeneous environments](https://arxiv.org/pdf/2206.01288)
* [swarm parallelism: training large models
can be surprisingly communication-efficient](https://arxiv.org/pdf/2301.11913)
* [a comprehensive survey of continual learning:
theory, method and application](https://arxiv.org/pdf/2302.00487)
* [model-agnostic meta-learning for fast adaptation of deep networks](https://arxiv.org/pdf/1703.03400)
* [the future of large language model pre-training is federated](https://arxiv.org/abs/2405.10853v2)
* [asynchronous local-sgd training for
language modeling](https://arxiv.org/pdf/2401.09135)
* [lo-fi: distributed fine-tuning without communication](https://arxiv.org/pdf/2210.11948)
* [decentralized training of foundation models in
heterogeneous environments](https://arxiv.org/pdf/2206.01288)
* [towards crowdsourced training of large neural
networks using decentralized mixture-of-experts](https://arxiv.org/pdf/2002.04013)

<br>

---

### models

<br>

* [deepseekmath: pushing the limits of mathematical reasoning in open language models](https://arxiv.org/abs/2402.03300)
* [training large language models to reason in a continuous latent space](https://arxiv.org/abs/2412.06769)
* [scaling up test-time compute with latent reasoning: a recurrent depth approach](https://arxiv.org/abs/2502.05171)
* [do all languages cost the same? tokenization in the era of commercial language models](https://arxiv.org/abs/2305.13707)

<br>

---

### 3d paralellism training and scaling

<br>

* [scaling up test-time compute with latent reasoning: a recurrent depth approach](https://arxiv.org/abs/2502.05171)
* [training large language models to reason in a continuous latent space](https://arxiv.org/abs/2412.06769)
* [a deep dive into 3d parallelism with nanotron](https://tj-solergibert.github.io/post/3d-parallelism/)

