## papers to read later (hopefully)

<br>

### ml on blockchains

<br>

* [opML: Optimistic Machine Learning on Blockchain](https://arxiv.org/pdf/2401.17555)

<br>

---

### parallel and decentralized training

<br>

* [swarm parallelism: training large models
can be surprisingly communication-efficient](https://arxiv.org/pdf/2301.11913)
* [deepmind's diloco: distributed low-communication
training of language models](https://arxiv.org/pdf/2311.08105)
* [decentralized training of foundation models in
heterogeneous environments](https://arxiv.org/pdf/2206.01288)
* [deepmind's dipaco: distributed path composition](https://arxiv.org/pdf/2403.10616)
* [towards crowdsourced training of large neural
networks using decentralized mixture-of-experts](https://arxiv.org/pdf/2002.04013)
* [deepmind's dipaco: distributed path composition](https://arxiv.org/pdf/2403.10616)
* [decentralized training of foundation models in
heterogeneous environments](https://arxiv.org/pdf/2206.01288)
* [swarm parallelism: training large models
can be surprisingly communication-efficient](https://arxiv.org/pdf/2301.11913)
* [a comprehensive survey of continual learning:
theory, method and application](https://arxiv.org/pdf/2302.00487)
* [model-agnostic meta-learning for fast adaptation of deep networks](https://arxiv.org/pdf/1703.03400)
* [the future of large language model pre-training is federated](https://arxiv.org/abs/2405.10853v2)
* [asynchronous local-sgd training for
language modeling](https://arxiv.org/pdf/2401.09135)
* [lo-fi: distributed fine-tuning without communication](https://arxiv.org/pdf/2210.11948)
* [decentralized training of foundation models in
heterogeneous environments](https://arxiv.org/pdf/2206.01288)
* [towards crowdsourced training of large neural
networks using decentralized mixture-of-experts](https://arxiv.org/pdf/2002.04013)

<br>

---

### 3d paralellism training

<br>

* [a deep dive into 3d parallelism with nanotron](https://tj-solergibert.github.io/post/3d-parallelism/)

